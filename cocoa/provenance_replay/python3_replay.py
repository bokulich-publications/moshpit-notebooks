#!/usr/bin/env python
# -----------------------------------------------------------------------------
# Auto-generated by qiime2 v.2024.10.1 at 10:59:07 AM on 29 Nov, 2024
# This document is a representation of the scholarly work of the creator of the
# QIIME 2 Results provided as input to this software, and may be protected by
# intellectual property law. Please respect all copyright restrictions and
# licenses governing the use, modification, and redistribution of this work.

# For User Support, post to the QIIME2 Forum at https://forum.qiime2.org.

# Instructions for use:
# 1. Open this script in a text editor or IDE. Support for Python
#    syntax highlighting is helpful.
# 2. Search or scan visually for '<' or '>' characters to find places where
#    user input (e.g. a filepath or column name) is required. If syntax
#    highlighting is enabled, '<' and '>' will appear as syntax errors.
# 3. Search for 'FIXME' comments in the script, and respond as directed.
# 4. Remove all 'FIXME' comments from the script completely. Failure to do so
#    may result in 'Missing Option' errors
# 5. Adjust the arguments to the commands below to suit your data and metadata.
#    If your data is not identical to that in the replayed analysis,
#    changes may be required. (e.g. sample ids or rarefaction depth)
# 6. Optional: search for 'SAVE' comments in the script, commenting out the
#    `some_result.save` lines for any Results you do not want saved to disk.
# 7. Activate your replay conda environment, and confirm you have installed all
#    plugins used by the script.
# 8. Run this script with `python <path to this script>`, or paste commands
#    into a python interpreter or jupyter notebook for an interactive analysis
# -----------------------------------------------------------------------------

from qiime2 import Artifact
from qiime2 import Metadata
import qiime2.plugins.assembly.actions as assembly_actions
import qiime2.plugins.diversity.actions as diversity_actions
import qiime2.plugins.fondue.actions as fondue_actions
import qiime2.plugins.moshpit.actions as moshpit_actions
import qiime2.plugins.motus.actions as motus_actions
import qiime2.plugins.sourmash.actions as sourmash_actions
import qiime2.plugins.taxa.actions as taxa_actions

ncbi_accession_i_ds_0 = Artifact.import_data(
    'NCBIAccessionIDs',
    <your data here>,
)
# SAVE: comment out the following with '# ' to skip saving this Result to disk
ncbi_accession_i_ds_0.save('ncbi_accession_i_ds_0')

kraken2_database_0, bracken_database_0 = moshpit_actions.build_kraken_db(
    collection='pluspf',
    threads=1,
    kmer_len=35,
    minimizer_len=31,
    minimizer_spaces=7,
    no_masking=False,
    max_db_size=0,
    use_ftp=False,
    load_factor=0.7,
    fast_build=False,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
kraken2_database_0.save('kraken2_database_0')
bracken_database_0.save('bracken_database_0')

busco_db_0, = moshpit_actions.fetch_busco_db(
    virus=False,
    prok=True,
    euk=False,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
busco_db_0.save('busco_db_0')

feature_table_frequency_0 = Artifact.import_data(
    'FeatureTable[Frequency]',
    <your data here>,
)
# SAVE: comment out the following with '# ' to skip saving this Result to disk
feature_table_frequency_0.save('feature_table_frequency_0')

database_0, = moshpit_actions.fetch_kaiju_db(
    database_type='nr_euk',
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
database_0.save('database_0')

reference_db_eggnog_0 = Artifact.import_data(
    'ReferenceDB[Eggnog]',
    <your data here>,
)
# SAVE: comment out the following with '# ' to skip saving this Result to disk
reference_db_eggnog_0.save('reference_db_eggnog_0')

reference_db_diamond_0 = Artifact.import_data(
    'ReferenceDB[Diamond]',
    <your data here>,
)
# SAVE: comment out the following with '# ' to skip saving this Result to disk
reference_db_diamond_0.save('reference_db_diamond_0')

_, _, paired_reads_0, _ = fondue_actions.get_all(
    accession_ids=ncbi_accession_i_ds_0,
    email='mziemski@ethz.ch',
    n_jobs=5,
    retries=5,
    log_level='DEBUG',
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
paired_reads_0.save('paired_reads_0')

reports_0, _ = moshpit_actions.classify_kraken2(
    seqs=paired_reads_0,
    kraken2_db=kraken2_database_0,
    threads=72,
    confidence=0.5,
    minimum_base_quality=0,
    memory_mapping=False,
    minimum_hit_groups=2,
    quick=False,
    report_minimizer_data=True,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
reports_0.save('reports_0')

contigs_0, = assembly_actions.assemble_megahit(
    seqs=paired_reads_0,
    presets='meta-sensitive',
    min_count=2,
    k_list=[21, 29, 39, 59, 79, 99, 119, 141],
    no_mercy=False,
    bubble_level=2,
    prune_level=2,
    prune_depth=2,
    disconnect_ratio=0.1,
    low_local_ratio=0.2,
    max_tip_len='auto',
    cleaning_rounds=5,
    no_local=False,
    kmin_1pass=False,
    memory=0.9,
    mem_flag=1,
    num_cpu_threads=24,
    no_hw_accel=False,
    min_contig_len=200,
    coassemble=False,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
contigs_0.save('contigs_0')

_, taxonomy_0 = moshpit_actions.classify_kaiju(
    seqs=paired_reads_0,
    db=database_0,
    z=16,
    a='greedy',
    e=3,
    m=11,
    s=65,
    evalue=0.01,
    x=True,
    r='species',
    c=0.1,
    exp=False,
    u=False,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
taxonomy_0.save('taxonomy_0')

table_0, taxonomy_1 = motus_actions.profile(
    samples=paired_reads_0,
    threads=4,
    min_alen=75,
    marker_gene_cutoff=3,
    mode='insert.scaled_counts',
    reference_genomes=False,
    jobs=4,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
taxonomy_1.save('taxonomy_1')
table_0.save('table_0')

_, taxonomy_2, table_1 = moshpit_actions.estimate_bracken(
    kraken_reports=reports_0,
    bracken_db=bracken_database_0,
    threshold=5,
    read_len=150,
    level='S',
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
taxonomy_2.save('taxonomy_2')
table_1.save('table_1')

index_0, = assembly_actions.index_contigs(
    contigs=contigs_0,
    large_index=False,
    debug=False,
    sanitized=False,
    verbose=True,
    noauto=False,
    packed=False,
    bmax='auto',
    bmaxdivn=4,
    dcv=1024,
    nodc=False,
    offrate=5,
    ftabchars=10,
    threads=8,
    seed=100,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
index_0.save('index_0')

filtered_table_0, = taxa_actions.filter_table(
    table=feature_table_frequency_0,
    taxonomy=taxonomy_0,
    exclude='unclassified,belong,cannot',
    query_delimiter=',',
    mode='contains',
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
filtered_table_0.save('filtered_table_0')

filtered_table_1, = taxa_actions.filter_table(
    table=table_0,
    taxonomy=taxonomy_1,
    exclude='u; n; a; s; s; i; g; n; e; d',
    query_delimiter=',',
    mode='contains',
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
filtered_table_1.save('filtered_table_1')

filtered_table_2, = taxa_actions.filter_table(
    table=table_1,
    taxonomy=taxonomy_2,
    exclude='Unclassified',
    query_delimiter=',',
    mode='contains',
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
filtered_table_2.save('filtered_table_2')

alignment_map_0, = assembly_actions.map_reads(
    index=index_0,
    reads=paired_reads_0,
    skip=0,
    qupto='unlimited',
    trim5=0,
    trim3=0,
    trim_to='untrimmed',
    phred33=False,
    phred64=False,
    mode='local',
    sensitivity='sensitive',
    n=0,
    len=22,
    i='S,1,1.15',
    n_ceil='L,0,0.15',
    dpad=15,
    gbar=4,
    ignore_quals=False,
    nofw=False,
    norc=False,
    no_1mm_upfront=False,
    end_to_end=False,
    local=False,
    ma=2,
    mp=6,
    np=1,
    rdg='5,3',
    rfg='5,3',
    k='off',
    a=False,
    d=15,
    r=2,
    minins=0,
    maxins=500,
    valid_mate_orientations='fr',
    no_mixed=False,
    no_discordant=False,
    dovetail=False,
    no_contain=False,
    no_overlap=False,
    offrate='off',
    threads=12,
    reorder=False,
    mm=False,
    seed=100,
    non_deterministic=False,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
alignment_map_0.save('alignment_map_0')

# Replay attempts to represent metadata inputs accurately, but metadata .tsv
# files are merged automatically by some interfaces, rendering distinctions
# between file inputs invisible in provenance. We output the recorded metadata
# to disk to enable visual inspection.

# The following command may have received additional metadata .tsv files. To
# confirm you have covered your metadata needs adequately, review the original
# metadata, saved at
# '/var/folders/7f/7nw_x13n5q965rss_qz6061m0000gq/T/tmpqp6st2e1/provenance_replay/recorded_metadata/diversity_core_metrics_0'

# NOTE: You may substitute already-loaded Metadata for the following, or cast a
# pandas.DataFrame to Metadata as needed.

metadata_0_md = Metadata.load(<your metadata filepath>)
action_results = diversity_actions.core_metrics(
    table=filtered_table_0,
    sampling_depth=5298000,
    metadata=metadata_0_md,
    with_replacement=False,
    n_jobs=6,
    ignore_missing_samples=False,
)
shannon_vector_0 = action_results.shannon_vector
# SAVE: comment out the following with '# ' to skip saving Results to disk
shannon_vector_0.save('shannon_vector_0')

# The following command may have received additional metadata .tsv files. To
# confirm you have covered your metadata needs adequately, review the original
# metadata, saved at
# '/var/folders/7f/7nw_x13n5q965rss_qz6061m0000gq/T/tmpqp6st2e1/provenance_replay/recorded_metadata/diversity_core_metrics_1'

# NOTE: You may substitute already-loaded Metadata for the following, or cast a
# pandas.DataFrame to Metadata as needed.

metadata_1_md = Metadata.load(<your metadata filepath>)
action_results = diversity_actions.core_metrics(
    table=filtered_table_1,
    sampling_depth=1792,
    metadata=metadata_1_md,
    with_replacement=False,
    n_jobs=6,
    ignore_missing_samples=False,
)
shannon_vector_1 = action_results.shannon_vector
# SAVE: comment out the following with '# ' to skip saving Results to disk
shannon_vector_1.save('shannon_vector_1')

# The following command may have received additional metadata .tsv files. To
# confirm you have covered your metadata needs adequately, review the original
# metadata, saved at
# '/var/folders/7f/7nw_x13n5q965rss_qz6061m0000gq/T/tmpqp6st2e1/provenance_replay/recorded_metadata/diversity_core_metrics_2'

# NOTE: You may substitute already-loaded Metadata for the following, or cast a
# pandas.DataFrame to Metadata as needed.

metadata_2_md = Metadata.load(<your metadata filepath>)
action_results = diversity_actions.core_metrics(
    table=filtered_table_2,
    sampling_depth=2320000,
    metadata=metadata_2_md,
    with_replacement=False,
    n_jobs=6,
    ignore_missing_samples=False,
)
rarefied_table_0 = action_results.rarefied_table
shannon_vector_2 = action_results.shannon_vector
# SAVE: comment out the following with '# ' to skip saving Results to disk
rarefied_table_0.save('rarefied_table_0')
shannon_vector_2.save('shannon_vector_2')

mags_0, _, _ = moshpit_actions.bin_contigs_metabat(
    contigs=contigs_0,
    alignment_maps=alignment_map_0,
    num_threads=64,
    seed=100,
    verbose=True,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
mags_0.save('mags_0')

results_table_0, _ = moshpit_actions.evaluate_busco(
    bins=mags_0,
    busco_db=busco_db_0,
    mode='genome',
    lineage_dataset='bacteria_odb10',
    augustus=False,
    auto_lineage=False,
    auto_lineage_euk=False,
    auto_lineage_prok=False,
    cpu=16,
    contig_break=10,
    evalue=0.001,
    force=False,
    limit=3,
    long=False,
    miniprot=False,
    scaffold_composition=False,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
results_table_0.save('results_table_0')

# The following command may have received additional metadata .tsv files. To
# confirm you have covered your metadata needs adequately, review the original
# metadata, saved at
# '/var/folders/7f/7nw_x13n5q965rss_qz6061m0000gq/T/tmpqp6st2e1/provenance_replay/recorded_metadata/moshpit_filter_mags_0'

results_table_0_a_0_md = results_table_0.view(Metadata)
filtered_mags_0, = moshpit_actions.filter_mags(
    mags=mags_0,
    metadata=results_table_0_a_0_md,
    where='complete>50',
    exclude_ids=False,
    on='mag',
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
filtered_mags_0.save('filtered_mags_0')

# The following command may have received additional metadata .tsv files. To
# confirm you have covered your metadata needs adequately, review the original
# metadata, saved at
# '/var/folders/7f/7nw_x13n5q965rss_qz6061m0000gq/T/tmpqp6st2e1/provenance_replay/recorded_metadata/moshpit_filter_mags_1'

filtered_mags_1, = moshpit_actions.filter_mags(
    mags=mags_0,
    metadata=results_table_0_a_0_md,
    where='complete>90',
    exclude_ids=False,
    on='mag',
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
filtered_mags_1.save('filtered_mags_1')

min_hash_signature_0, = sourmash_actions.compute(
    sequence_file=filtered_mags_0,
    ksizes=35,
    scaled=10,
    track_abundance=True,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
min_hash_signature_0.save('min_hash_signature_0')

min_hash_signature_1, = sourmash_actions.compute(
    sequence_file=filtered_mags_1,
    ksizes=35,
    scaled=10,
    track_abundance=True,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
min_hash_signature_1.save('min_hash_signature_1')

compare_output_0, = sourmash_actions.compare(
    min_hash_signature=min_hash_signature_0,
    ksize=35,
    ignore_abundance=True,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
compare_output_0.save('compare_output_0')

compare_output_1, = sourmash_actions.compare(
    min_hash_signature=min_hash_signature_1,
    ksize=35,
    ignore_abundance=True,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
compare_output_1.save('compare_output_1')

dereplicated_mags_0, _ = moshpit_actions.dereplicate_mags(
    mags=filtered_mags_0,
    distance_matrix=compare_output_0,
    threshold=0.99,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
dereplicated_mags_0.save('dereplicated_mags_0')

dereplicated_mags_1, _ = moshpit_actions.dereplicate_mags(
    mags=filtered_mags_1,
    distance_matrix=compare_output_1,
    threshold=0.99,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
dereplicated_mags_1.save('dereplicated_mags_1')

lengths_0, = moshpit_actions.get_feature_lengths(
    features=dereplicated_mags_0,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
lengths_0.save('lengths_0')

index_1, = assembly_actions.index_derep_mags(
    mags=dereplicated_mags_0,
    large_index=False,
    debug=False,
    sanitized=False,
    verbose=True,
    noauto=False,
    packed=False,
    bmax='auto',
    bmaxdivn=4,
    dcv=1024,
    nodc=False,
    offrate=5,
    ftabchars=10,
    threads=8,
    seed=100,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
index_1.save('index_1')

reports_1, hits_0 = moshpit_actions.classify_kraken2(
    seqs=dereplicated_mags_0,
    kraken2_db=kraken2_database_0,
    threads=72,
    confidence=0.5,
    minimum_base_quality=0,
    memory_mapping=False,
    minimum_hit_groups=2,
    quick=False,
    report_minimizer_data=True,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
hits_0.save('hits_0')
reports_1.save('reports_1')

index_2, = assembly_actions.index_derep_mags(
    mags=dereplicated_mags_1,
    large_index=False,
    debug=False,
    sanitized=False,
    verbose=True,
    noauto=False,
    packed=False,
    bmax='auto',
    bmaxdivn=4,
    dcv=1024,
    nodc=False,
    offrate=5,
    ftabchars=10,
    threads=8,
    seed=100,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
index_2.save('index_2')

eggnog_hits_0, _ = moshpit_actions.eggnog_diamond_search(
    sequences=dereplicated_mags_1,
    diamond_db=reference_db_diamond_0,
    num_cpus=16,
    db_in_memory=True,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
eggnog_hits_0.save('eggnog_hits_0')

lengths_1, = moshpit_actions.get_feature_lengths(
    features=dereplicated_mags_1,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
lengths_1.save('lengths_1')

alignment_map_1, = assembly_actions.map_reads(
    index=index_1,
    reads=paired_reads_0,
    skip=0,
    qupto='unlimited',
    trim5=0,
    trim3=0,
    trim_to='untrimmed',
    phred33=False,
    phred64=False,
    mode='local',
    sensitivity='sensitive',
    n=0,
    len=22,
    i='S,1,1.15',
    n_ceil='L,0,0.15',
    dpad=15,
    gbar=4,
    ignore_quals=False,
    nofw=False,
    norc=False,
    no_1mm_upfront=False,
    end_to_end=False,
    local=False,
    ma=2,
    mp=6,
    np=1,
    rdg='5,3',
    rfg='5,3',
    k='off',
    a=False,
    d=15,
    r=2,
    minins=0,
    maxins=500,
    valid_mate_orientations='fr',
    no_mixed=False,
    no_discordant=False,
    dovetail=False,
    no_contain=False,
    no_overlap=False,
    offrate='off',
    threads=12,
    reorder=False,
    mm=False,
    seed=100,
    non_deterministic=False,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
alignment_map_1.save('alignment_map_1')

taxonomy_3, = moshpit_actions.kraken2_to_mag_features(
    reports=reports_1,
    hits=hits_0,
    coverage_threshold=0.1,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
taxonomy_3.save('taxonomy_3')

alignment_map_2, = assembly_actions.map_reads(
    index=index_2,
    reads=paired_reads_0,
    skip=0,
    qupto='unlimited',
    trim5=0,
    trim3=0,
    trim_to='untrimmed',
    phred33=False,
    phred64=False,
    mode='local',
    sensitivity='sensitive',
    n=0,
    len=22,
    i='S,1,1.15',
    n_ceil='L,0,0.15',
    dpad=15,
    gbar=4,
    ignore_quals=False,
    nofw=False,
    norc=False,
    no_1mm_upfront=False,
    end_to_end=False,
    local=False,
    ma=2,
    mp=6,
    np=1,
    rdg='5,3',
    rfg='5,3',
    k='off',
    a=False,
    d=15,
    r=2,
    minins=0,
    maxins=500,
    valid_mate_orientations='fr',
    no_mixed=False,
    no_discordant=False,
    dovetail=False,
    no_contain=False,
    no_overlap=False,
    offrate='off',
    threads=12,
    reorder=False,
    mm=False,
    seed=100,
    non_deterministic=False,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
alignment_map_2.save('alignment_map_2')

ortholog_annotations_0, = moshpit_actions.eggnog_annotate(
    eggnog_hits=eggnog_hits_0,
    eggnog_db=reference_db_eggnog_0,
    db_in_memory=True,
    num_cpus=16,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
ortholog_annotations_0.save('ortholog_annotations_0')

abundances_0, = moshpit_actions.estimate_mag_abundance(
    maps=alignment_map_1,
    mag_lengths=lengths_0,
    metric='rpkm',
    min_mapq=42,
    min_query_len=0,
    min_base_quality=20,
    min_read_len=0,
    threads=24,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
abundances_0.save('abundances_0')

abundances_1, = moshpit_actions.estimate_mag_abundance(
    maps=alignment_map_2,
    mag_lengths=lengths_1,
    metric='rpkm',
    min_mapq=42,
    min_query_len=0,
    min_base_quality=20,
    min_read_len=0,
    threads=24,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
abundances_1.save('abundances_1')

annotation_frequency_0, = moshpit_actions.extract_annotations(
    ortholog_annotations=ortholog_annotations_0,
    annotation='caz',
    max_evalue=0.0001,
    min_score=0.0,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
annotation_frequency_0.save('annotation_frequency_0')

# The following command may have received additional metadata .tsv files. To
# confirm you have covered your metadata needs adequately, review the original
# metadata, saved at
# '/var/folders/7f/7nw_x13n5q965rss_qz6061m0000gq/T/tmpqp6st2e1/provenance_replay/recorded_metadata/diversity_core_metrics_3'

# NOTE: You may substitute already-loaded Metadata for the following, or cast a
# pandas.DataFrame to Metadata as needed.

metadata_5_md = Metadata.load(<your metadata filepath>)
action_results = diversity_actions.core_metrics(
    table=abundances_0,
    sampling_depth=210,
    metadata=metadata_5_md,
    with_replacement=False,
    n_jobs=6,
    ignore_missing_samples=False,
)
shannon_vector_3 = action_results.shannon_vector
rarefied_table_1 = action_results.rarefied_table
# SAVE: comment out the following with '# ' to skip saving Results to disk
shannon_vector_3.save('shannon_vector_3')
rarefied_table_1.save('rarefied_table_1')

result_table_0, = moshpit_actions.multiply_tables(
    table1=abundances_1,
    table2=annotation_frequency_0,
)
# SAVE: comment out the following with '# ' to skip saving Results to disk
result_table_0.save('result_table_0')

# The following command may have received additional metadata .tsv files. To
# confirm you have covered your metadata needs adequately, review the original
# metadata, saved at
# '/var/folders/7f/7nw_x13n5q965rss_qz6061m0000gq/T/tmpqp6st2e1/provenance_replay/recorded_metadata/diversity_core_metrics_4'

# NOTE: You may substitute already-loaded Metadata for the following, or cast a
# pandas.DataFrame to Metadata as needed.

metadata_6_md = Metadata.load(<your metadata filepath>)
action_results = diversity_actions.core_metrics(
    table=result_table_0,
    sampling_depth=8000,
    metadata=metadata_6_md,
    with_replacement=False,
    n_jobs=6,
    ignore_missing_samples=False,
)
rarefied_table_2 = action_results.rarefied_table
shannon_vector_4 = action_results.shannon_vector
# SAVE: comment out the following with '# ' to skip saving Results to disk
rarefied_table_2.save('rarefied_table_2')
shannon_vector_4.save('shannon_vector_4')


# -----------------------------------------------------------------------------
# The following QIIME 2 Results were parsed to produce this script:
# 137046cb-fe18-4d0b-babc-de209fb49e65 	 2b5122a9-0396-4ab7-966b-f66fa8a00705
# 5a0bf237-2368-4fe7-9901-50d9d6d92dfb 	 6cc70426-844c-47ec-82ee-9be5a2d90cda
# 7f0e9ecb-01f7-4a4c-b38f-0bee505d5f76 	 af746e01-9abf-4d0f-89bb-8abe68b38705
# b6ac8052-ee05-4029-b871-6b2202641a86 	 bf3224d3-1051-47f4-b79f-c1997f22e1b6
# ca4c6fe6-66a7-41ca-b434-bab6f1cc6d18 	 e2c284d9-3b8b-4de8-8c19-dc090a0a07e0
# -----------------------------------------------------------------------------
