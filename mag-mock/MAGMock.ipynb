{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f6977b-dbf3-42f0-bf3a-41a5c883cec6",
   "metadata": {},
   "source": [
    "# Mock community analysis using the MOSHPIT suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466be291-77ed-45fb-bf13-44c36bb539dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import subprocess\n",
    "import uuid\n",
    "import urllib.request\n",
    "import tempfile\n",
    "\n",
    "import qiime2 as q2\n",
    "from qiime2.plugins import (\n",
    "    rescript, feature_table, assembly, \n",
    "    moshpit, sourmash, taxa as taxa_plugin\n",
    ")\n",
    "\n",
    "from utils._utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580a3fe-ee7f-4755-a169-243fc170eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # proxy support, in case needed\n",
    "# urllib.request.install_opener(\n",
    "#     urllib.request.build_opener(\n",
    "#         urllib.request.ProxyHandler(\n",
    "#             {'http' : os.environ.get('http_proxy'), \n",
    "#              'https': os.environ.get('https_proxy')}\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6720846-1d6a-4588-8206-b2efc315a82e",
   "metadata": {},
   "source": [
    "Variables and constants used throughout this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90a3f6-8bd9-4113-9e2b-32a9b8e27ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS = 24\n",
    "SEED = 100\n",
    "TOTAL_READS = 20_000_000\n",
    "READ_LEN = 150\n",
    "\n",
    "# define sample names (keys) and their respective parameters \n",
    "SAMPLES = {\n",
    "    \"uni20\": {\"profile\": \"uniform\", \"num_reads\": TOTAL_READS},\n",
    "    \"exp20\": {\"profile\": \"exponential\", \"num_reads\": TOTAL_READS},\n",
    "    \"log20\": {\"profile\": \"lognormal\", \"num_reads\": TOTAL_READS},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4ded6-f304-4fe6-b581-51ec9e3e70f9",
   "metadata": {},
   "source": [
    "Directories used throughout this notebook - all directories will be created automatically, if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8de0a-6d89-4758-9d75-eb37e0ea4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "genomes_dir = os.path.join(data_dir, \"individual\")\n",
    "simulated_reads_dir = os.path.join(data_dir, \"reads\")\n",
    "\n",
    "for d in [genomes_dir, simulated_reads_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "cache = q2.Cache(os.path.join(data_dir, \"cache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a386208-e62a-4d0a-8496-81dc6dfba7a5",
   "metadata": {},
   "source": [
    "## Fetch reference sequences\n",
    "Start by defining a list of species which will be used to construct the mock community. Every taxon will get a UUID - this is required later in the pipeline when we generate MAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155f1b4-2a1e-4d19-a494-58c4113fd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa = {\n",
    "    \"paer\": {\n",
    "        \"taxon\": \"Pseudomonas aeruginosa\",\n",
    "        \"uuid\": uuid.uuid4()\n",
    "    },\n",
    "    \"ecol_k12\": {\n",
    "        \"taxon\": \"511145\", #Escherichia coli str. K-12 substr. MG1655\n",
    "        \"uuid\": uuid.uuid4()\n",
    "    },\n",
    "    \"ecol_o157\": {\n",
    "        \"taxon\": \"Escherichia coli O157:H7 str. Sakai DNA\",\n",
    "        \"uuid\": uuid.uuid4()\n",
    "    },\n",
    "    \"sent\": {\n",
    "        \"taxon\": \"Salmonella enterica\",\n",
    "        \"uuid\": uuid.uuid4()\n",
    "    },\n",
    "    \"saur\": {\n",
    "        \"taxon\": \"Staphylococcus aureus\",\n",
    "        \"uuid\": uuid.uuid4()\n",
    "    },\n",
    "    \"lmon\": {\n",
    "        \"taxon\": \"Listeria monocytogenes\",\n",
    "        \"uuid\": uuid.uuid4()\n",
    "    },\n",
    "    \"bsub\": {\n",
    "        \"taxon\": \"Bacillus subtilis\",\n",
    "        \"uuid\": uuid.uuid4()\n",
    "    },\n",
    "    \"mtub\": {\n",
    "        \"taxon\": \"Mycobacterium tuberculosis\",\n",
    "        \"uuid\": uuid.uuid4()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1cbb0a-7e6c-4ab7-96d8-7aa69aea256a",
   "metadata": {},
   "source": [
    "Use [RESCRIPt](https://github.com/bokulich-lab/RESCRIPt) plugin to fetch the sequences defined above. The `get-ncbi-genomes` method fetches genome sequences, corresponding taxonomies, protein and gene annotations - we will only keep the first two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b9c6c-b07b-4626-bc7b-f1c6f3efe048",
   "metadata": {},
   "outputs": [],
   "source": [
    "genomes_all, taxonomies_all, to_remove = [], [], []\n",
    "if not os.path.isfile(os.path.join(data_dir, \"cache\", \"keys\", \"ref_taxonomy\")):\n",
    "    ids = {}\n",
    "    for abbrev, inner_dict in taxa.items():\n",
    "        taxon = inner_dict[\"taxon\"]\n",
    "        _id = inner_dict[\"uuid\"]\n",
    "        \n",
    "        directory = f\"{genomes_dir}/{abbrev}\"\n",
    "        \n",
    "        print(f\"Fetching genome for {taxon}...\")\n",
    "        (genome, _, _, taxonomy) = rescript.methods.get_ncbi_genomes(\n",
    "            taxon=taxon,\n",
    "            assembly_source=\"refseq\",\n",
    "            assembly_levels=[\"complete_genome\"],\n",
    "            only_genomic=True,\n",
    "            only_reference=True\n",
    "        )\n",
    "    \n",
    "        genome_fp = os.path.join(genomes_dir, f\"{abbrev}.qza\")\n",
    "        genome.save(genome_fp)\n",
    "        with tempfile.TemporaryDirectory() as tmp:\n",
    "            genome.export_data(tmp)\n",
    "            \n",
    "            src = os.path.join(tmp, \"dna-sequences.fasta\")\n",
    "            dst = os.path.join(genomes_dir, f\"{_id}.fasta\")\n",
    "            shutil.move(src, dst)\n",
    "    \n",
    "            with open(dst, \"r\") as f:\n",
    "                accession_id = f.readline().split()[0][1:]\n",
    "            \n",
    "            ids[accession_id] = inner_dict\n",
    "            \n",
    "        genomes_all.append(genome)\n",
    "        \n",
    "        taxonomy_fp = os.path.join(genomes_dir, f\"{abbrev}_taxonomy.qza\")\n",
    "        taxonomy.save(taxonomy_fp)\n",
    "        taxonomies_all.append(taxonomy)\n",
    "    \n",
    "        to_remove.extend([genome_fp, taxonomy_fp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68539ed5-a1d5-4fad-b0d2-0d100e0d1139",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Merge all the sequence files into a single artifact (repeat for the taxonomies) and save (we may need those later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473eb4e2-0a25-444b-b25b-bf78f8b8aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ref_taxonomy = cache.load(\"ref_taxonomy\")\n",
    "except KeyError:\n",
    "\n",
    "    merged_seqs, = feature_table.methods.merge_seqs(data=genomes_all)\n",
    "    merged_seqs.save(os.path.join(data_dir, f\"genomes.qza\"))\n",
    "    \n",
    "    merged_taxonomies, = feature_table.methods.merge_taxa(data=taxonomies_all)\n",
    "    merged_taxonomies.save(os.path.join(data_dir, f\"taxonomy.qza\"));\n",
    "    \n",
    "    # update taxonomy with the new ids\n",
    "    merged_taxonomies_ser = merged_taxonomies.view(pd.Series)\n",
    "    merged_taxonomies_ser.index = merged_taxonomies_ser.index.map(\n",
    "        {x: y[\"uuid\"] for x, y in ids.items()}\n",
    "    )\n",
    "    \n",
    "    ref_taxonomy = q2.Artifact.import_data(\n",
    "        \"FeatureData[Taxonomy]\", merged_taxonomies_ser\n",
    "    )\n",
    "    cache.save(ref_taxonomy, \"ref_taxonomy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f07431d-3884-4370-9efd-abd49f3ebec4",
   "metadata": {},
   "source": [
    "Clean up the `individual` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d08ef-581a-4883-b84b-c89138fc30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in to_remove:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be691e-34c7-44e5-98ac-72ba839540b5",
   "metadata": {},
   "source": [
    "Import the individual genomes into the `FeatureData[MAG]` artifact - we will use those later as our reference MAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3615c51-9a3c-40cd-af3f-3b4f72e851d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ref_genomes = cache.load(\"ref_genomes\")\n",
    "except KeyError:\n",
    "    with tempfile.TemporaryDirectory() as tmp:\n",
    "        for f in glob.glob(os.path.join(genomes_dir, \"*.fasta\")):\n",
    "            shutil.copy(\n",
    "                f, os.path.join(tmp, os.path.basename(f))\n",
    "            )\n",
    "            \n",
    "        ref_genomes = q2.Artifact.import_data(\"FeatureData[MAG]\", tmp)\n",
    "        cache.save(ref_genomes, \"ref_genomes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ddaab2-2eee-49e2-ad0d-0d232d83182d",
   "metadata": {},
   "source": [
    "## Simulate reads\n",
    "Use the reference genomes to simulate a sequencing experiment according to the `SAMPLES` dictionary defined on top of the notebook. We first generate the abundance profiles, as per our spec, and then use the `mason_simulator` from the [SeqAn](https://github.com/seqan/seqan) library to generate reads from the references according to the abundance profle. Finally, we import all the samples and abundances into a QIIME 2 artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b9747-acbb-4965-9e50-9d5f6496faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if reads are already available\n",
    "try:\n",
    "    reads = cache.load(\"reads\")\n",
    "except KeyError:\n",
    "    reads = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af526c6a-b486-4367-9c03-34b3379f36b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if reads is None:\n",
    "    abundances_all = []\n",
    "    for sample_name, sample_details in SAMPLES.items():\n",
    "        df = simulate_reads(\n",
    "            genomes_dir=genomes_dir, \n",
    "            total_reads=sample_details[\"num_reads\"], \n",
    "            abundance_profile=sample_details[\"profile\"], \n",
    "            sample_name=sample_name, \n",
    "            simulated_reads_dir=simulated_reads_dir,\n",
    "            threads=THREADS, read_len=READ_LEN, seed=SEED,\n",
    "        )\n",
    "        abundances_all.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d81e3a-9eb1-4d7c-8954-12996ad48e0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Clean up the indices generated during read simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda11236-8509-41d1-a383-922a6f918474",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in glob.glob(os.path.join(genomes_dir, \"*.fasta.fai\")):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75070182-5e97-4d48-a8a4-d9188fbb6c86",
   "metadata": {},
   "source": [
    "Construct the concatenated abundance table from all the samples and import into QIIME 2 artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8dcbe5-b5c5-48a6-8eba-00bc969d2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reads is None:\n",
    "    abundances_all = pd.concat(abundances_all, axis=1)\n",
    "    abundances_artifact = q2.Artifact.import_data(\n",
    "        \"FeatureTable[RelativeFrequency]\", abundances_all\n",
    "    )\n",
    "    cache.save(abundances_artifact, \"abundances\")\n",
    "else:\n",
    "    abundances_artifact = cache.load(\"abundances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e19354-d7af-4d7a-a9e1-1449c9ff5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "abundances_artifact.view(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30902a81-4bf2-4a53-a135-a3e25a2b5340",
   "metadata": {},
   "source": [
    "Finally, import the reads into a QIIME 2 artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbec544-9c1b-4226-a509-acc889479e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reads is None:\n",
    "    reads = q2.Artifact.import_data(\n",
    "        \"SampleData[PairedEndSequencesWithQuality]\",\n",
    "        simulated_reads_dir,\n",
    "        \"CasavaOneEightSingleLanePerSampleDirFmt\",\n",
    "    )\n",
    "    cache.save(reads, \"reads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5c5b7-e703-4f84-9a66-ce5183c8d341",
   "metadata": {},
   "source": [
    "## Metagenome assembly\n",
    "In this section we use the simulated reads to reconstruct the genomes which they originated from. We use actions from [q2-assembly](https://github.com/bokulich-lab/q2-assembly.git) and [q2-moshpit](https://github.com/bokulich-lab/q2-moshpit.git) plugins to assemble contigs, bin them into MAGs, filter the high quality MAGs and dereplicate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3ca35-b01e-4295-ba9c-b18b174b20a4",
   "metadata": {},
   "source": [
    "### Contig assembly\n",
    "We begin by using MEGAHIT as our assembler of choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb9d858-6e7b-49df-8549-885714872b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    contigs = cache.load(\"contigs\")\n",
    "except KeyError:\n",
    "    contigs, = assembly.pipelines.assemble_megahit(\n",
    "        seqs=reads,\n",
    "        presets=\"meta-sensitive\",\n",
    "        num_cpu_threads=THREADS,\n",
    "    )\n",
    "    cache.save(contigs, \"contigs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b14181-f994-4959-9f92-0ad732568f76",
   "metadata": {},
   "source": [
    "Next, we index the contigs and map the reads using Bowtie 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f78b0-efb2-4545-875c-ee4bd9a3ba19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    contigs_index = cache.load(\"contigs_index\")\n",
    "except KeyError:\n",
    "    contigs_index, = assembly.pipelines.index_contigs(\n",
    "        contigs=contigs,\n",
    "        threads=THREADS,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    cache.save(contigs_index, \"contigs_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300216ff-dab2-4b03-8b01-e3ba0c7c986f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    reads_to_contigs = cache.load(\"reads_to_contigs\")\n",
    "except KeyError:\n",
    "    reads_to_contigs, = assembly.pipelines.map_reads(\n",
    "        index=contigs_index,\n",
    "        reads=reads,\n",
    "        threads=THREADS,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    cache.save(reads_to_contigs, \"reads_to_contigs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f784a-9e0d-4aa8-b688-f5f73d419c77",
   "metadata": {},
   "source": [
    "### Binning\n",
    "We use the alignment maps to generate MAGs with MetaBat 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744884d4-860d-4b93-b20a-6dfbd21af997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mags = cache.load(\"mags\")\n",
    "    contig_map = cache.load(\"contig_map\")\n",
    "    contigs_unbinned = cache.load(\"contigs_unbinned\")\n",
    "except:\n",
    "    (mags, contig_map, contigs_unbinned) = moshpit.methods.bin_contigs_metabat(\n",
    "        contigs=contigs,\n",
    "        alignment_maps=reads_to_contigs,\n",
    "        num_threads=THREADS,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    cache.save(mags, \"mags\")\n",
    "    cache.save(contig_map, \"contig_map\")\n",
    "    cache.save(contigs_unbinned, \"contigs_unbinned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56304c9-edeb-441d-b379-994d0a8cfa71",
   "metadata": {},
   "source": [
    "To evaluate the quality of resulting MAGs we can use BUSCO. We start by fetching the prokaryotic BUSCO database, which we then use to estimate BUSCO counts in the recovered MAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcfe45f-df64-4842-a6cd-0a5fda5d9343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    busco_db = cache.load(\"busco_db\")\n",
    "except KeyError:\n",
    "    busco_db,  = moshpit.methods.fetch_busco_db(prok=True,)\n",
    "    cache.save(busco_db, \"busco_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78438786-1b3b-44f3-bdd6-cdfa55a29370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    busco_results = cache.load(\"busco_results\")\n",
    "    busco_viz = q2.Visualization.load(os.path.join(data_dir, \"mags.qzv\"))\n",
    "except:\n",
    "    (busco_results, busco_vis) = moshpit.pipelines.evaluate_busco(\n",
    "        bins=mags,\n",
    "        busco_db=busco_db,\n",
    "        lineage_dataset=\"bacteria_odb10\",\n",
    "        cpu=THREADS\n",
    "    )\n",
    "    cache.save(busco_results, \"busco_results\")\n",
    "    busco_vis.save(os.path.join(data_dir, \"mags.qzv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c2a68-9c07-4d57-b66d-a2603d3f127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2.Visualization.load(os.path.join(data_dir, \"mags.qzv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbff953-2450-4dbb-b337-321fc666fac8",
   "metadata": {},
   "source": [
    "### MAG quality filtering\n",
    "Before we continue with the analysis, we filter the MAGs based on their quality - we only want to retain those which were labeled by BUSCO as at least 90% complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b248b-0dbb-4133-88a4-ad0f86111b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mags_filtered = cache.load(\"mags_filtered\")\n",
    "except KeyError:\n",
    "    mags_filtered, = moshpit.methods.filter_mags(\n",
    "        mags=mags,\n",
    "        metadata=busco_results.view(q2.Metadata),\n",
    "        where=\"complete>90\",\n",
    "        on=\"mag\",\n",
    "    )\n",
    "    cache.save(mags_filtered, \"mags_filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3940f4-a5b9-4a4d-848a-a3e9c9143e2b",
   "metadata": {},
   "source": [
    "### MAG dereplication\n",
    "In order to generate a dereplicated set of MAGs we will need a distance matrix. We can obtain it using the Sourmash tool: we first generate MinHash signatures of our MAGs and compare them to one another - this results in a distance matrix which we then input to the dereplication action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b2e7f-ae89-4ce1-a89a-d1675882ed6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mags_hashes = cache.load(\"mags_hashes\")\n",
    "except KeyError:\n",
    "    mags_hashes, = sourmash.methods.compute(\n",
    "        sequence_file=mags_filtered,\n",
    "        ksizes=51,\n",
    "        scaled=10000\n",
    "    )\n",
    "    cache.save(mags_hashes, \"mags_hashes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdef02e-223f-4678-8c76-006345b74144",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mags_dist = cache.load(\"mags_dist\")\n",
    "except KeyError:\n",
    "    mags_dist, = sourmash.methods.compare(\n",
    "        min_hash_signature=mags_hashes,\n",
    "        ksize=51\n",
    "    )\n",
    "    cache.save(mags_dist, \"mags_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f85df-4d78-4db4-93f5-585a8a0df3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mags_derep = cache.load(\"mags_derep\")\n",
    "    mags_pa = cache.load(\"mags_pa\")\n",
    "except KeyError:\n",
    "    (mags_derep, mags_pa) = moshpit.methods.dereplicate_mags(\n",
    "        mags=mags_filtered,\n",
    "        distance_matrix=mags_dist,\n",
    "        threshold=0.98\n",
    "    )\n",
    "    cache.save(mags_derep, \"mags_derep\")\n",
    "    cache.save(mags_pa, \"mags_pa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e98bb8-a88b-4b73-8565-9db49f9ff6b8",
   "metadata": {},
   "source": [
    "### MAG abundance estimation\n",
    "We try to recover abundances of each MAG by mapping reads to the dereplicated MAGs and using the RPKM/TPM metrics as a proxy for abundance. We begin by indexing the MAGs, followed by read mapping and, finally, abundance estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03612da-4747-428e-bcda-383c039c819d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mags_derep_index = cache.load(\"mags_derep_index\")\n",
    "except KeyError:\n",
    "    mags_derep_index, = assembly.methods.index_derep_mags(\n",
    "        mags=mags_derep,\n",
    "        threads=THREADS,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    cache.save(mags_derep_index, \"mags_derep_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625cd147-efe0-42d6-9524-bbbe457d3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    reads_to_derep_mags = cache.load(\"reads_to_derep_mags\")\n",
    "except KeyError:\n",
    "    reads_to_derep_mags, = assembly.pipelines.map_reads(\n",
    "        index=mags_derep_index,\n",
    "        reads=reads,\n",
    "        threads=THREADS,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    cache.save(reads_to_derep_mags, \"reads_to_derep_mags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df2a30-082e-4b3a-b2be-a214976de0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mags_derep_length = cache.load(\"mags_derep_length\")\n",
    "except KeyError:\n",
    "    mags_derep_length, = moshpit.methods.get_feature_lengths(\n",
    "        features=mags_derep,\n",
    "    )\n",
    "    cache.save(mags_derep_length, \"mags_derep_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41bdd6-8b8e-4bf6-94f5-076406014868",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mags_rpkm = cache.load(\"mags_rpkm\")\n",
    "    mags_tpm = cache.load(\"mags_tpm\")\n",
    "except KeyError:\n",
    "    mags_rpkm, = moshpit.methods.estimate_mag_abundance(\n",
    "        mag_lengths=mags_derep_length,\n",
    "        maps=reads_to_derep_mags,\n",
    "        threads=THREADS,\n",
    "        metric=\"rpkm\",\n",
    "        min_mapq=42\n",
    "    )\n",
    "    mags_tpm, = moshpit.methods.estimate_mag_abundance(\n",
    "        mag_lengths=mags_derep_length,\n",
    "        maps=reads_to_derep_mags,\n",
    "        threads=THREADS,\n",
    "        metric=\"tpm\",\n",
    "        min_mapq=42\n",
    "    )\n",
    "    cache.save(mags_tpm, \"mags_rpkm\")\n",
    "    cache.save(mags_rpkm, \"mags_tpm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c050666-9e43-4fa5-a9aa-88a9b7961a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mags_rpkm.view(pd.DataFrame).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23462530-ef3e-4094-8c7a-825d3cd4f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mags_tpm.view(pd.DataFrame).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e770ba-5677-40aa-8fda-87e7338d6700",
   "metadata": {},
   "source": [
    "## Taxonomic classification\n",
    "The MOSHPIT pipeline supports a variety of ways to perform taxonomic classification. Here, we will use Kraken 2 as our classifier of choice and classify both, reads and recovered dereplicated MAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ab10a-1416-4ef5-81f4-2e25d3edd7bc",
   "metadata": {},
   "source": [
    "### Databases\n",
    "Fetch databases required for the classification actions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b649dc-b754-449f-8c32-c844a31381c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kraken_db = cache.load(\"kraken_db\")\n",
    "    bracken_db = cache.load(\"bracken_db\")\n",
    "except KeyError:\n",
    "    (kraken_db, bracken_db) = moshpit.methods.build_kraken_db(\n",
    "        collection=\"pluspf8\"\n",
    "    )\n",
    "    cache.save(kraken_db, \"kraken_db\")\n",
    "    cache.save(bracken_db, \"bracken_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13814bd7-35c0-48cc-a9af-5d88bac5d097",
   "metadata": {},
   "source": [
    "### Classification of reads\n",
    "We use Kraken 2 to classify reads against the PlusPF database, followed by Bracken's abundance re-estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691e09c-710b-4343-99c6-07931c172666",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kraken_reports_reads = cache.load(\"kraken_reports_reads\")\n",
    "    kraken_hits_reads = cache.load(\"kraken_hits_reads\")\n",
    "except KeyError:\n",
    "    (kraken_reports_reads, kraken_hits_reads) = moshpit.pipelines.classify_kraken2(\n",
    "        seqs=reads,\n",
    "        kraken2_db=kraken_db,\n",
    "        threads=2*THREADS,\n",
    "        memory_mapping=True\n",
    "    )\n",
    "    cache.save(kraken_reports_reads, \"kraken_reports_reads\")\n",
    "    cache.save(kraken_hits_reads, \"kraken_hits_reads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef41c1-6091-48d0-95fa-454b9db0f477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    bracken_reports = cache.load(\"bracken_reports\")\n",
    "    bracken_taxonomy = cache.load(\"bracken_taxonomy\")\n",
    "    bracken_ft = cache.load(\"bracken_ft\")\n",
    "except KeyError:\n",
    "    (bracken_reports, bracken_taxonomy, bracken_ft) = moshpit.methods.estimate_bracken(\n",
    "        kraken_reports=kraken_reports_reads,\n",
    "        bracken_db=bracken_db,\n",
    "        read_len=READ_LEN,\n",
    "        level=\"S\"\n",
    "    )\n",
    "    cache.save(bracken_reports, \"bracken_reports\")\n",
    "    cache.save(bracken_taxonomy, \"bracken_taxonomy\")\n",
    "    cache.save(bracken_ft, \"bracken_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887c141-e191-4071-afe0-e33c3bbe05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    barplot_reads = q2.Visualization.load(os.path.join(data_dir, \"reads-barplot.qzv\"))\n",
    "except:\n",
    "    barplot_reads, =  taxa_plugin.visualizers.barplot(\n",
    "        table=bracken_ft,\n",
    "        taxonomy=bracken_taxonomy\n",
    "    )\n",
    "    barplot_reads.save(os.path.join(data_dir, \"reads-barplot.qzv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e38a38-0c8b-4013-9f96-70433a1af9d9",
   "metadata": {},
   "source": [
    "### Classification of dereplicated MAGs\n",
    "We use the same database to classify the dereplicated MAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df1b46-beda-4e81-9adf-3543a533c628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    kraken_reports_derep = cache.load(\"kraken_reports_derep\")\n",
    "    kraken_hits_derep = cache.load(\"kraken_hits_derep\")\n",
    "except KeyError:\n",
    "    (kraken_reports_derep, kraken_hits_derep) = moshpit.pipelines.classify_kraken2(\n",
    "        seqs=mags_derep,\n",
    "        kraken2_db=kraken_db,\n",
    "        threads=2*THREADS,\n",
    "        memory_mapping=True\n",
    "    )\n",
    "    cache.save(kraken_reports_derep, \"kraken_reports_derep\")\n",
    "    cache.save(kraken_hits_derep, \"kraken_hits_derep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6238e4-39ba-498a-8bcd-c074a2571ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mags_derep_taxonomy = cache.load(\"mags_derep_taxonomy\")\n",
    "except KeyError:\n",
    "    mags_derep_taxonomy, = moshpit.methods.kraken2_to_mag_features(\n",
    "        reports=kraken_reports_derep,\n",
    "        hits=kraken_hits_derep,\n",
    "        coverage_threshold=10\n",
    "    )\n",
    "    cache.save(mags_derep_taxonomy, \"mags_derep_taxonomy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897733bc-5f30-4436-9b3b-9a6401e1bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in mags_derep_taxonomy.view(pd.Series).items():\n",
    "    print(f\"{i}\\n{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18527d-2690-4be2-b642-9d4bab177fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    barplot_derep = q2.Visualization.load(os.path.join(data_dir, \"mags-rpkm-barplot.qzv\"))\n",
    "except:\n",
    "    barplot_derep, =  taxa_plugin.visualizers.barplot(\n",
    "        table=mags_rpkm,\n",
    "        taxonomy=mags_derep_taxonomy\n",
    "    )\n",
    "    barplot_derep.save(os.path.join(data_dir, \"mags-rpkm-barplot.qzv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c92f9-57b8-4283-a66a-eb261ba59311",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2.Visualization.load(os.path.join(data_dir, \"mags-rpkm-barplot.qzv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c818597-59bb-4f27-8d72-9ac449a06a04",
   "metadata": {},
   "source": [
    "## Abundance comparison\n",
    "We can use the abundance estimation action to calculate the abundance of the reference genomes by mapping the simulated reads to the reference we obtained from NCBI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f852f-39e9-4bb0-bd07-3d773a5a7c08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ref_genomes_index = cache.load(\"ref_genomes_index\")\n",
    "except KeyError:\n",
    "    ref_genomes_index, = assembly.methods.index_derep_mags(\n",
    "        mags=ref_genomes,\n",
    "        threads=THREADS,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    cache.save(ref_genomes_index, \"ref_genomes_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae63ef-b83b-4d3d-848e-3561ed3beeec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    reads_to_ref_genomes = cache.load(\"reads_to_ref_genomes\")\n",
    "except KeyError:\n",
    "    reads_to_ref_genomes, = assembly.pipelines.map_reads(\n",
    "        index=ref_genomes_index,\n",
    "        reads=reads,\n",
    "        threads=THREADS,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    cache.save(reads_to_ref_genomes, \"reads_to_ref_genomes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de973875-036c-4f23-93f1-65583abecca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ref_genomes_length = cache.load(\"ref_genomes_length\")\n",
    "except KeyError:\n",
    "    ref_genomes_length, = moshpit.methods.get_feature_lengths(\n",
    "        features=ref_genomes,\n",
    "    )\n",
    "    cache.save(ref_genomes_length, \"ref_genomes_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55437f22-7ede-4d41-9c87-fbcff3eff26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ref_genomes_rpkm = cache.load(\"ref_genomes_rpkm\")\n",
    "except KeyError:\n",
    "    ref_genomes_rpkm, = moshpit.methods.estimate_mag_abundance(\n",
    "        mag_lengths=ref_genomes_length,\n",
    "        maps=reads_to_ref_genomes,\n",
    "        threads=THREADS,\n",
    "        metric=\"rpkm\",\n",
    "        min_mapq=42\n",
    "    )\n",
    "    cache.save(ref_genomes_rpkm, \"ref_genomes_rpkm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81054798-cd39-4560-ade4-d82ed2a4bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    barplot_ref_genomes = q2.Visualization.load(\n",
    "        os.path.join(data_dir, \"ref-genomes-rpkm-barplot.qzv\")\n",
    "    )\n",
    "except:\n",
    "    barplot_ref_genomes, =  taxa_plugin.visualizers.barplot(\n",
    "        table=ref_genomes_rpkm,\n",
    "        taxonomy=ref_taxonomy\n",
    "    )\n",
    "    barplot_ref_genomes.save(os.path.join(data_dir, \"ref-genomes-rpkm-barplot.qzv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d4811-ac7d-45eb-b7c6-9a41a2dc3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2.Visualization.load(os.path.join(data_dir, \"ref-genomes-rpkm-barplot.qzv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2231f4-8c19-4bf5-b45d-b1872ac45d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T19:51:32.928484Z",
     "iopub.status.busy": "2024-06-12T19:51:32.927039Z",
     "iopub.status.idle": "2024-06-12T19:51:32.933876Z",
     "shell.execute_reply": "2024-06-12T19:51:32.932621Z",
     "shell.execute_reply.started": "2024-06-12T19:51:32.928409Z"
    }
   },
   "source": [
    "## Functional annotation\n",
    "In this section we try to identify genes in the dereplicated MAGs and annotate them using EggNOG. We start by fetch the complete Diamond database and use it to identify ortholog candidates in our set of genomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609104e-f80f-407a-860b-4e62e40e3602",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    diamond_db = cache.load(\"diamond_db\")\n",
    "except KeyError:\n",
    "    diamond_db,  = moshpit.methods.fetch_diamond_db()\n",
    "    cache.save(diamond_db, \"diamond_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44516d-97ad-4377-b52a-5da539e383d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    eggnog_hits = cache.load(\"eggnog_hits\")\n",
    "    eggnog_ftf = cache.load(\"eggnog_ftf\")\n",
    "except:\n",
    "    eggnog_hits, eggnog_ftf = moshpit.pipelines.eggnog_diamond_search(\n",
    "        sequences=mags_derep,\n",
    "        diamond_db=diamond_db,\n",
    "        num_cpus=THREADS,\n",
    "        db_in_memory=True\n",
    "    )\n",
    "    cache.save(eggnog_hits, \"eggnog_hits\")\n",
    "    cache.save(eggnog_ftf, \"eggnog_ftf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dd9a4-b152-408f-aebc-aec55ddb0e9d",
   "metadata": {},
   "source": [
    "We fetch the EggNOG annotation database and perform the annotation of orthologs identified by Diamond in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb22af-270c-4b08-a2be-58a68143a0a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    eggnog_db = cache.load(\"eggnog_db\")\n",
    "except KeyError:\n",
    "    eggnog_db,  = moshpit.methods.fetch_eggnog_db()\n",
    "    cache.save(eggnog_db, \"eggnog_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16108250-8f04-4157-b2cf-ef6379f26342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ortholog_annotations = cache.load(\"ortholog_annotations\")\n",
    "except KeyError:\n",
    "    ortholog_annotations, = moshpit.pipelines.eggnog_annotate(\n",
    "        eggnog_hits=eggnog_hits,\n",
    "        eggnog_db=eggnog_db,\n",
    "        num_cpus=THREADS,\n",
    "        db_in_memory=True\n",
    "    )\n",
    "    cache.save(ortholog_annotations, \"ortholog_annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277d082-98bb-40e9-8887-61288e938d3e",
   "metadata": {},
   "source": [
    "The previous step generated annotation tables for each MAG which we can now convert into feature tables. We pick `kegg_pathways` as our annotation of choice and run the `extract_annotations` action to extract those and expand them into a feature table containing pathway counts (MAGs x KEGG pathways)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e112e88-199a-42d8-aa5a-65b9869047c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cazymes = cache.load(\"cazymes\")\n",
    "except KeyError:\n",
    "    cazymes, = moshpit.methods.extract_annotations(\n",
    "        ortholog_annotations=ortholog_annotations,\n",
    "        annotation=\"caz\",\n",
    "        max_evalue=0.0001\n",
    "    )\n",
    "    cache.save(cazymes, \"cazymes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2117d57-3519-41a4-9a06-978117a79f11",
   "metadata": {},
   "source": [
    "Finally, to obtain the count of each pathway per sample (sample x KEGG pathway) we can calculate the dot product of the MAG abundance table with the table from the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8849da-f1c2-4d53-9c2e-8b823fab2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cazymes_ft = cache.load(\"cazymes_ft\")\n",
    "except KeyError:\n",
    "    cazymes_ft, = moshpit.pipelines.multiply_tables(\n",
    "        table1=mags_tpm,\n",
    "        table2=cazymes\n",
    "    )\n",
    "    cache.save(cazymes_ft, \"cazymes_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3133e-b7b5-4763-88e8-a4a4439fc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    barplot_cazymes = q2.Visualization.load(\n",
    "        os.path.join(data_dir, \"cazymes-barplot.qzv\")\n",
    "    )\n",
    "except:\n",
    "    barplot_cazymes, =  taxa_plugin.visualizers.barplot(\n",
    "        table=cazymes_ft,\n",
    "    )\n",
    "    barplot_cazymes.save(os.path.join(data_dir, \"cazymes-barplot.qzv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8308e3c-cb66-4efd-811f-1f6ac3f1c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2.Visualization.load(os.path.join(data_dir, \"cazymes-barplot.qzv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "q2-metagenome-2024.10",
   "language": "python",
   "name": "q2-metagenome-2024.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
